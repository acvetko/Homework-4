{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In building a model for this data, first I am going to import the sets and look for missing variables. The missing variables will be filled with what makes sense in their context and given their correlations. Then I will do feature engineering to 1) make sure all variables are coded as the correct type and 2) scale variables by numer of people in the household and age where appropriate. Then I will subset down to the head of households to train my model, (because head of households are what is the important target), and finally I will run a number of models to categorize the households into their poverty classifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(r\"C:\\Users\\owner\\Documents\\Junior\\S2\\Machine_Learning\\HW4\\train.csv\")\n",
    "test=pd.read_csv(r\"C:\\Users\\owner\\Documents\\Junior\\S2\\Machine_Learning\\HW4\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23856, 142)\n",
      "(9557, 143)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to put my training and testing data in the same set so I can make sure all missing values are taken into account and that the two sets have the same number of columns in the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain=train.shape[0]\n",
    "ntest=test.shape[0]\n",
    "test['Target']=0\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>v2a1</th>\n",
       "      <th>hacdor</th>\n",
       "      <th>rooms</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>v18q</th>\n",
       "      <th>v18q1</th>\n",
       "      <th>r4h1</th>\n",
       "      <th>...</th>\n",
       "      <th>SQBescolari</th>\n",
       "      <th>SQBage</th>\n",
       "      <th>SQBhogar_total</th>\n",
       "      <th>SQBedjefe</th>\n",
       "      <th>SQBhogar_nin</th>\n",
       "      <th>SQBovercrowding</th>\n",
       "      <th>SQBdependency</th>\n",
       "      <th>SQBmeaned</th>\n",
       "      <th>agesq</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_279628684</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>1849</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1849</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_f29eb3ddd</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>4489</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>4489</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_68de51c94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>8464</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>8464</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_d671db89c</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>289</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>289</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_d56d6f5f5</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>1369</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ID_ec05b1a7b</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>1444</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1444</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ID_e9e0c1100</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ID_3e04e571e</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>16</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ID_1284f8aad</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>900</td>\n",
       "      <td>16</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>900</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ID_51f52fdd2</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>784</td>\n",
       "      <td>16</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id      v2a1  hacdor  rooms  hacapo  v14a  refrig  v18q  v18q1  \\\n",
       "0  ID_279628684  190000.0       0      3       0     1       1     0    NaN   \n",
       "1  ID_f29eb3ddd  135000.0       0      4       0     1       1     1    1.0   \n",
       "2  ID_68de51c94       NaN       0      8       0     1       1     0    NaN   \n",
       "3  ID_d671db89c  180000.0       0      5       0     1       1     1    1.0   \n",
       "4  ID_d56d6f5f5  180000.0       0      5       0     1       1     1    1.0   \n",
       "5  ID_ec05b1a7b  180000.0       0      5       0     1       1     1    1.0   \n",
       "6  ID_e9e0c1100  180000.0       0      5       0     1       1     1    1.0   \n",
       "7  ID_3e04e571e  130000.0       1      2       0     1       1     0    NaN   \n",
       "8  ID_1284f8aad  130000.0       1      2       0     1       1     0    NaN   \n",
       "9  ID_51f52fdd2  130000.0       1      2       0     1       1     0    NaN   \n",
       "\n",
       "   r4h1   ...    SQBescolari  SQBage  SQBhogar_total  SQBedjefe  SQBhogar_nin  \\\n",
       "0     0   ...            100    1849               1        100             0   \n",
       "1     0   ...            144    4489               1        144             0   \n",
       "2     0   ...            121    8464               1          0             0   \n",
       "3     0   ...             81     289              16        121             4   \n",
       "4     0   ...            121    1369              16        121             4   \n",
       "5     0   ...            121    1444              16        121             4   \n",
       "6     0   ...              4      64              16        121             4   \n",
       "7     0   ...              0      49              16         81             4   \n",
       "8     0   ...             81     900              16         81             4   \n",
       "9     0   ...            121     784              16         81             4   \n",
       "\n",
       "   SQBovercrowding  SQBdependency  SQBmeaned  agesq  Target  \n",
       "0         1.000000            0.0      100.0   1849       4  \n",
       "1         1.000000           64.0      144.0   4489       4  \n",
       "2         0.250000           64.0      121.0   8464       4  \n",
       "3         1.777778            1.0      121.0    289       4  \n",
       "4         1.777778            1.0      121.0   1369       4  \n",
       "5         1.777778            1.0      121.0   1444       4  \n",
       "6         1.777778            1.0      121.0     64       4  \n",
       "7        16.000000            1.0      100.0     49       4  \n",
       "8        16.000000            1.0      100.0    900       4  \n",
       "9        16.000000            1.0      100.0    784       4  \n",
       "\n",
       "[10 rows x 143 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rez_esc            0.825457\n",
       "v18q1              0.762218\n",
       "v2a1               0.726154\n",
       "meaneduc           0.001077\n",
       "SQBmeaned          0.001077\n",
       "techozinc          0.000000\n",
       "techoentrepiso     0.000000\n",
       "techocane          0.000000\n",
       "techootro          0.000000\n",
       "cielorazo          0.000000\n",
       "abastaguadentro    0.000000\n",
       "sanitario3         0.000000\n",
       "abastaguafuera     0.000000\n",
       "abastaguano        0.000000\n",
       "public             0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data=(all_data.isnull().sum()/len(all_data))\n",
    "missing_data.sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a list of all the variables with missing data. Luckily, there's only 5. I will fill them in order. From the data description, rez_esc is the \"years behind in school\". I want to learn more about this variable before I decide how to deal with it, like if it's NaN if the person didn't attend school, or if it seems like it's NaN if the person is 0 years behind in school. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4348422496570645\n",
      "1.5817481806773257\n",
      "0.0\n",
      "99.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(all_data[\"rez_esc\"]))\n",
    "print(np.std(all_data[\"rez_esc\"]))\n",
    "print(np.min(all_data[\"rez_esc\"]))\n",
    "print(np.max(all_data[\"rez_esc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rez_esc</th>\n",
       "      <th>escolari</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rez_esc  escolari\n",
       "0      NaN        10\n",
       "1      NaN        12\n",
       "2      NaN        11\n",
       "3      1.0         9\n",
       "4      NaN        11\n",
       "5      NaN        11\n",
       "6      0.0         2\n",
       "7      0.0         0\n",
       "8      NaN         9\n",
       "9      NaN        11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[[\"rez_esc\", \"escolari\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21530368472725836\n",
      "0.033162783793879984\n",
      "testing_rez     1.000000\n",
      "age             0.471585\n",
      "SQBage          0.392844\n",
      "agesq           0.392844\n",
      "parentesco1     0.306266\n",
      "estadocivil3    0.280629\n",
      "SQBescolari     0.243302\n",
      "parentesco2     0.217754\n",
      "escolari        0.215304\n",
      "instlevel8      0.187381\n",
      "dtype: float64\n",
      "rez_esc       1.000000\n",
      "agesq         0.235885\n",
      "SQBage        0.235885\n",
      "age           0.224549\n",
      "instlevel3    0.132524\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "all_data[\"testing_rez\"]=np.where(all_data[\"rez_esc\"].isnull(), 1, 0)\n",
    "print(all_data[\"testing_rez\"].corr(all_data[\"escolari\"]))\n",
    "print(all_data[\"testing_rez\"].corr(all_data[\"meaneduc\"]))\n",
    "print(pd.DataFrame.corrwith(all_data, all_data[\"testing_rez\"]).sort_values(ascending=False).head(10))\n",
    "print(pd.DataFrame.corrwith(all_data, all_data[\"rez_esc\"]).sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rez_esc is missing in 83% of the observations. It's not correlated with any of the education variables, and it's absense isn't correlated with any variables. It ranges from 0 to 99. Maybe I could fill it in with the average of the region, but that isn't highly correlated. Years of schooling has no missing data and is sufficient to cover anything that could be predicted with rez_esc, so I will just drop it. Next I will look to fill v18q1. \"v18q1\" is the number of tablets a household owns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(['rez_esc'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.364002517306482\n",
      "0.7144384108611518\n",
      "1.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(all_data[\"v18q1\"]))\n",
    "print(np.std(all_data[\"v18q1\"]))\n",
    "print(np.min(all_data[\"v18q1\"]))\n",
    "print(np.max(all_data[\"v18q1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   v18q  v18q1\n",
      "0     0    NaN\n",
      "1     1    1.0\n",
      "2     0    NaN\n",
      "3     1    1.0\n",
      "4     1    1.0\n",
      "   testv18q1  v18q\n",
      "0          1     0\n",
      "1          0     1\n",
      "2          1     0\n",
      "3          0     1\n",
      "4          0     1\n",
      "-0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(all_data[[\"v18q\", \"v18q1\"]].head())\n",
    "all_data[\"testv18q1\"]=np.where(all_data['v18q1'].isnull(), 1, 0)\n",
    "print(all_data[[\"testv18q1\", \"v18q\"]].head())\n",
    "print(all_data[\"testv18q1\"].corr(all_data[\"v18q\"]))\n",
    "all_data.drop(['testv18q1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty clear that a null value for the number of tablets a family had means that the family has 0 tablets. First, the number of tablets a family has does not go down to zero already. Second, having a null value for the number of tablets a family has is nearly perfectly correlated with them saying that they did not have a tablet in the pervious question. So I'm just going to fill in the null values with zero. Then I will look at \"v2a1\", which is the variable for monthly rent payment, which has 73% of its values missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"v18q1\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172030.8455737705 154995.03127164065 2852700.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(all_data[\"v2a1\"]), np.std(all_data[\"v2a1\"]), np.max(all_data[\"v2a1\"]), np.min(all_data[\"v2a1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2a1           1.000000\n",
      "rooms          0.453555\n",
      "meaneduc       0.401386\n",
      "SQBmeaned      0.375273\n",
      "bedrooms       0.344225\n",
      "SQBedjefe      0.333262\n",
      "v18q1          0.328233\n",
      "SQBescolari    0.319282\n",
      "cielorazo      0.301601\n",
      "v18q           0.288995\n",
      "dtype: float64\n",
      "pisocemento       -0.258056\n",
      "overcrowding      -0.240206\n",
      "tipovivi3         -0.232605\n",
      "epared2           -0.213151\n",
      "eviv2             -0.207117\n",
      "SQBovercrowding   -0.203074\n",
      "energcocinar3     -0.189155\n",
      "etecho2           -0.182297\n",
      "area2             -0.178532\n",
      "paredpreb         -0.158911\n",
      "dtype: float64\n",
      "testv2a1         1.000000\n",
      "tipovivi1        0.790821\n",
      "tipovivi5        0.186518\n",
      "hogar_mayor      0.183686\n",
      "area2            0.178311\n",
      "agesq            0.168695\n",
      "SQBage           0.168695\n",
      "age              0.157075\n",
      "elimbasu3        0.121721\n",
      "energcocinar4    0.116172\n",
      "dtype: float64\n",
      "tipovivi3   -0.734760\n",
      "tipovivi2   -0.556836\n",
      "area1       -0.178311\n",
      "meaneduc    -0.160948\n",
      "elimbasu1   -0.153089\n",
      "cielorazo   -0.141501\n",
      "SQBmeaned   -0.140617\n",
      "SQBedjefe   -0.139557\n",
      "lugar1      -0.121780\n",
      "v18q        -0.101099\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame.corrwith(all_data, all_data[\"v2a1\"]).sort_values(ascending=False).head(10))\n",
    "print(pd.DataFrame.corrwith(all_data, all_data[\"v2a1\"]).sort_values(ascending=True).head(10))\n",
    "all_data[\"testv2a1\"]=np.where(all_data[\"v2a1\"].isnull(), 1, 0)\n",
    "print(pd.DataFrame.corrwith(all_data, all_data[\"testv2a1\"]).sort_values(ascending=False).head(10))\n",
    "print(pd.DataFrame.corrwith(all_data, all_data[\"testv2a1\"]).sort_values(ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above correlations, it is clear that having a \"NaN\" value for monthly payments is highly correlated with owning a home, and having another (assigned or borrowed) home status. So all the people who own homes which are fully paid off can have a 0 for their monthly rent payment. Additionally, homes which are \"assigned or borrowed\", (which is also highly correlated with a missing rent value), can have 0. This deals with most of the missing variables in the v2a1 column. I'm going to go ahead and fill the remaining missing values in with the means from the region, because I believe it's practical to think that the remaining 2% of missing values are similar across regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"v2a1\"]=np.where((all_data[\"v2a1\"].isnull() & all_data[\"tipovivi1\"]==1), 0, all_data[\"v2a1\"])\n",
    "all_data[\"v2a1\"]=np.where((all_data[\"v2a1\"].isnull() & all_data[\"tipovivi5\"]==1), 0, all_data[\"v2a1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    190000.0\n",
       "1    135000.0\n",
       "2         0.0\n",
       "3    180000.0\n",
       "4    180000.0\n",
       "Name: v2a1, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[\"v2a1\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Interpreting tuple 'by' as a list of keys, rather than a single key. Use 'by=[...]' instead of 'by=(...)'. In the future, a tuple will always mean a single key.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_data[\"v2a1\"] = all_data.groupby((\"lugar1\", \"lugar2\", \"lugar3\", \"lugar4\", \"lugar5\", \"lugar6\"))[\"v2a1\"].transform(\n",
    "    lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've dealt with the missing values in v2a1, I will look at my last missing values: meaneduc and SQBmeaned, both of which have .1% of values missing. Because SQBmeaned is just the squareroot of meaneduc, I will fill in what I can for meaneduc and then fill the squareroots in for SQBmeaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.17866594238228 4.105663801258728 37.0 0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(all_data[\"meaneduc\"]), np.std(all_data[\"meaneduc\"]), np.max(all_data[\"meaneduc\"]), np.min(all_data[\"meaneduc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testmeaneduc     1.000000\n",
      "SQBdependency    0.152053\n",
      "elimbasu4        0.044101\n",
      "sanitario5       0.039640\n",
      "energcocinar1    0.038107\n",
      "tipovivi4        0.030004\n",
      "noelec           0.029514\n",
      "paredmad         0.029389\n",
      "tipovivi3        0.028983\n",
      "sanitario1       0.028002\n",
      "dtype: float64\n",
      "hogar_adul    -0.074426\n",
      "v14a          -0.064534\n",
      "r4t2          -0.039733\n",
      "r4t3          -0.038335\n",
      "hogar_total   -0.038233\n",
      "hhsize        -0.038233\n",
      "tamhog        -0.038233\n",
      "rooms         -0.036863\n",
      "refrig        -0.035471\n",
      "r4h3          -0.034837\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "all_data[\"testmeaneduc\"]=np.where(all_data[\"meaneduc\"].isnull(), 1, 0)\n",
    "print(pd.DataFrame.corrwith(all_data, all_data[\"testmeaneduc\"]).sort_values(ascending=False).head(10))\n",
    "print(pd.DataFrame.corrwith(all_data, all_data[\"testmeaneduc\"]).sort_values(ascending=True).head(10))\n",
    "all_data.drop(['testmeaneduc'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data for meaneduc doesn't seemto be correlated with anything. It's only missing a few observations, so I'm going to fill null observations in with the mean of the region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Interpreting tuple 'by' as a list of keys, rather than a single key. Use 'by=[...]' instead of 'by=(...)'. In the future, a tuple will always mean a single key.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_data[\"meaneduc\"] = all_data.groupby((\"lugar1\", \"lugar2\", \"lugar3\", \"lugar4\", \"lugar5\", \"lugar6\"))[\"meaneduc\"].transform(\n",
    "    lambda x: x.fillna(x.median()))\n",
    "\n",
    "all_data[\"SQBmeaned\"]=np.where(all_data[\"SQBmeaned\"].isnull(), np.sqrt(all_data[\"meaneduc\"]), all_data[\"SQBmeaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testv2a1           0.0\n",
       "testing_rez        0.0\n",
       "sanitario5         0.0\n",
       "sanitario3         0.0\n",
       "sanitario2         0.0\n",
       "sanitario1         0.0\n",
       "coopele            0.0\n",
       "noelec             0.0\n",
       "planpri            0.0\n",
       "public             0.0\n",
       "abastaguano        0.0\n",
       "abastaguafuera     0.0\n",
       "abastaguadentro    0.0\n",
       "cielorazo          0.0\n",
       "techootro          0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data=(all_data.isnull().sum()/len(all_data))\n",
    "missing_data.sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33413, 144)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to make sure all data has the correct type and generate new variables and generate variables to measure overall home quality, and scale variables by number of people in the household and age where appropriate. \n",
    "First, dependency is the number of dependents to working-aged people. It should be a float64 rather than an integer, so I am going to fill the \"yes\" and \"no\" with 1 and 0 respectively so I can make it the correct type. The same goes with edjefa and edjefe. The data description on Kaggle even says that Yes=1 and no=0 for these variables, so I feel comfortable making the substitutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"yes\": 1, \"no\": 0}\n",
    "all_data['dependency'] = all_data['dependency'].replace(mapping).astype(np.float64)\n",
    "all_data['edjefa'] = all_data['edjefa'].replace(mapping).astype(np.float64)\n",
    "all_data['edjefe'] = all_data['edjefe'].replace(mapping).astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in the number of people in a household and the household size may be indicative of poverty level because it can take into account overcrowding in the home. Additionally,  general variables describing the overall quality of the home aggregated up could be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['hhsize-diff'] = all_data['tamviv'] - all_data['hhsize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['walls'] = np.argmax(np.array(all_data[['epared1', 'epared2', 'epared3']]), axis=1)\n",
    "all_data['roof'] = np.argmax(np.array(all_data[['etecho1', 'etecho2', 'etecho3']]), axis=1)\n",
    "all_data['floor'] = np.argmax(np.array(all_data[['eviv1', 'eviv2', 'eviv3']]), axis=1)\n",
    "all_data['walls+roof+floor'] = all_data['walls'] + all_data['roof'] + all_data['floor']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'm going to put in variables to measure overall quality of life from material goods and scale possessions by number of people in the home and by age of the individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['nogood'] = 1 * (all_data['sanitario1'] +  \n",
    "                         all_data['pisonotiene'] + \n",
    "                         all_data['abastaguano'] + \n",
    "                         (all_data['cielorazo'] == 0))\n",
    "all_data['bonus'] = 1 * (all_data['refrig'] + \n",
    "                      all_data['computer'] + \n",
    "                      (all_data['v18q1'] > 0) + \n",
    "                      all_data['television'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Household commodities should be scaled by the number of people in each household to provide the best measure of comparision across observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['phones-per-capita'] = all_data['qmobilephone'] / all_data['tamviv']\n",
    "all_data['tablets-per-capita'] = all_data['v18q1'] / all_data['tamviv']\n",
    "all_data['rooms-per-capita'] = all_data['rooms'] / all_data['tamviv']\n",
    "all_data['rent-per-capita'] = all_data['v2a1'] / all_data['tamviv']\n",
    "all_data['escolari/age'] = all_data['escolari'] / all_data['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['tech'] = all_data['v18q'] + all_data['mobilephone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I am going to make a dictioinary of variables which should be categorized as boolean, continuous, or ordered so I can ensure they are used properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = ['Id', 'idhogar', 'Target']\n",
    "hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n",
    "           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n",
    "           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n",
    "           'pisonatur', 'pisonotiene', 'pisomadera',\n",
    "           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n",
    "           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n",
    "            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n",
    "           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n",
    "           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n",
    "           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n",
    "           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n",
    "           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n",
    "           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n",
    "           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n",
    "           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']\n",
    "\n",
    "hh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n",
    "              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin','hhsize-diff',\n",
    "              'walls', 'roof', 'floor', 'walls+roof+floor', 'nogood', 'bonus',\n",
    "              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n",
    "\n",
    "hh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding',\n",
    "          'phones-per-capita', 'tablets-per-capita', 'rooms-per-capita', 'rent-per-capita']\n",
    "ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n",
    "            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n",
    "            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n",
    "            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n",
    "            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n",
    "            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n",
    "            'instlevel9', 'mobilephone']\n",
    "\n",
    "ind_ordered = ['age', 'escolari',  'tech']\n",
    "\n",
    "ind_cont = ['escolari/age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in (hh_bool + ind_bool):\n",
    "    all_data[variable] = all_data[variable].astype('bool')\n",
    "for variable in (hh_cont + ind_cont):\n",
    "    all_data[variable] = all_data[variable].astype(float) \n",
    "for variable in (hh_ordered + ind_ordered):\n",
    "        all_data[variable] = all_data[variable].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33413, 157)\n"
     ]
    }
   ],
   "source": [
    "all_data[\"parentesco1\"].head()\n",
    "print((all_data).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to check and make sure that there is no more missing data after the feature engineering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "escolari/age     0.010625\n",
       "sanitario2       0.000000\n",
       "energcocinar4    0.000000\n",
       "energcocinar3    0.000000\n",
       "energcocinar2    0.000000\n",
       "energcocinar1    0.000000\n",
       "sanitario6       0.000000\n",
       "sanitario5       0.000000\n",
       "sanitario3       0.000000\n",
       "sanitario1       0.000000\n",
       "elimbasu2        0.000000\n",
       "coopele          0.000000\n",
       "noelec           0.000000\n",
       "planpri          0.000000\n",
       "public           0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data=(all_data.isnull().sum()/len(all_data))\n",
    "missing_data.sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "escolari/age has missing data, but that's from a divide by zero error. I'll go ahead and fill those with 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"escolari/age\"]=np.where(all_data[\"escolari/age\"].isnull(), 0, all_data[\"escolari/age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tech             0.0\n",
       "sanitario1       0.0\n",
       "energcocinar3    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data=(all_data.isnull().sum()/len(all_data))\n",
    "missing_data.sort_values(ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because only the heads of household need to be accurately predicted, I'm only going to train on the heads of the household. This way I can maximize my predictive power on the correct targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "1    4\n",
       "2    4\n",
       "5    4\n",
       "8    4\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = all_data.loc[all_data['parentesco1'] == 1, :]\n",
    "heads['Target'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "heads.drop(['Id'], axis=1, inplace=True)\n",
    "heads.drop(['idhogar'], axis=1, inplace=True)\n",
    "heads=pd.get_dummies(heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2973, 155) (23856, 155)\n"
     ]
    }
   ],
   "source": [
    "train = heads.loc[all_data['Target'] != 0, :]\n",
    "test = all_data.loc[all_data['Target']==0, :]\n",
    "Id=test['Id']\n",
    "test.drop(['Id'], axis=1, inplace=True)\n",
    "test.drop(['idhogar'], axis=1, inplace=True)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model I'm going to try will be a Random Forest because it's typically a very strong model and it will be a good benchmark to see how I'm doing with my data set. I will hypertune the parameters for number of trees, maximum depth, and class weight, (because there is not a uniform distribution of households across the different poverty classifications). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=train['Target']\n",
    "train = train.drop('Target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test=train_test_split(train, target, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'max_depth': 8, 'class_weight': 'balanced_subsample'}\n",
      "Best Negative MSE: 0.4158769832756503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_dis= {\"n_estimators\": [70, 100, 120, 150], \"max_depth\": [5, 6, 8, 9], \"class_weight\": [\"balanced\", \"balanced_subsample\", None]}\n",
    "clf=RandomForestClassifier()\n",
    "n_inter=10\n",
    "random_search=RandomizedSearchCV(clf, param_distributions=param_dis, n_iter=n_inter, cv=4, scoring='f1_macro')\n",
    "\n",
    "random_search.fit(x_train, y_train)\n",
    "print(\"Best Parameters: {}\".format(random_search.best_params_))\n",
    "print(\"Best Negative MSE: {}\".format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the random forest model maximises the f1_macro score when it has 100 estimators, a max depth of 5, and uses a balanced subsample. It makes sense that the model results in a balanced subsample, because there was a huge inbalance in the classes, with most households being classfied as 4s and fewest being classified as 1s. I'm going to run the random forest model again, and this time use my tuned hyper parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1: 0.7835382006807614\n",
      "Test f1: 0.43129996003197446\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=100, max_depth=8, class_weight=\"balanced_subsample\")\n",
    "clf.fit(x_train, y_train)\n",
    "train_predictions = clf.predict(x_train)\n",
    "test_predictions = clf.predict(x_test)\n",
    "\n",
    "train_f1 = f1_score(y_train, train_predictions, average='macro')\n",
    "test_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "\n",
    "print(\"Train f1: {}\".format(train_f1))\n",
    "print(\"Test f1: {}\".format(test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's definitley overfitting, but the test f1 scoreis still relativley good, so I'm going to submit the model to kaggle and see how it goes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop('Target', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_clf=clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_clf=pd.DataFrame()\n",
    "sub_clf['Id']=Id\n",
    "sub_clf['target']=prediction_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_clf.to_csv(\"sub5_clf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle score actually wasn't too bad. The f1_macro was 0.427. I'll have that be my personal benchmark for now and work with more models to see if I can fix the overfitting and get a better score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next model I will try is a Gradient Boosting Model. Gradient boosting has trees develop sequencially rather than independently, (like Random Forest), so this model seems to be a natural next step. I'll see if the sequencial development works better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'tol': 0.001, 'n_estimators': 150, 'learning_rate': 1}\n",
      "Best Negative MSE: 0.3731007934512792\n"
     ]
    }
   ],
   "source": [
    "bgc=GradientBoostingClassifier()\n",
    "param_dis= {\"learning_rate\": [.01, .1, 1], \"n_estimators\": [75, 100, 150], \"tol\": [.01, .1, .001]}\n",
    "n_inter=10\n",
    "random_search=RandomizedSearchCV(bgc, param_distributions=param_dis, n_iter=n_inter, cv=4, scoring='f1_macro')\n",
    "\n",
    "random_search.fit(x_train, y_train)\n",
    "print(\"Best Parameters: {}\".format(random_search.best_params_))\n",
    "print(\"Best Negative MSE: {}\".format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1: 1.0\n",
      "Test f1: 0.366903762859382\n"
     ]
    }
   ],
   "source": [
    "bgc=GradientBoostingClassifier(tol=0.001, n_estimators=100, learning_rate=1)\n",
    "bgc.fit(x_train, y_train)\n",
    "train_predictions = bgc.predict(x_train)\n",
    "test_predictions = bgc.predict(x_test)\n",
    "\n",
    "train_f1 = f1_score(y_train, train_predictions, average='macro')\n",
    "test_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "\n",
    "print(\"Train f1: {}\".format(train_f1))\n",
    "print(\"Test f1: {}\".format(test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model operates best with a tolerance of 0.001, 150 trees, and a learning rate of 1. While my last model was overfitting, this one is definitley overfitting worse. There is a huge bag betweenthe training and testing f1, and the training data is predicted perfectly. I could perhaps mitigate the overfitting by decreasing the number of trees, but even this optomized model does not do as well as the random forest, (as demonstrated below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_BGC=bgc.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_bgc=pd.DataFrame()\n",
    "sub_bgc['Id']=Id\n",
    "sub_bgc['target']=prediction_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_bgc.to_csv(\"sub1_bgc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Kaggle score was 0.413, not as good as the Random Forest. So I'm going to continue with one final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use SVM for my final model, which will trian a hypterplan to identify different classes. I chose this model because it has such a different approach than the decision trees I used in my previous two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'C': 0.05, 'gamma': 0.5, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "svc=SVC()\n",
    "gridsearch = GridSearchCV(svc, {\"C\": [0.05, 0.1, 0.01], \"kernel\": ['rbf'], 'gamma': [0.5, .75, .25]}, scoring='f1_macro')\n",
    "gridsearch.fit(x_train, y_train)\n",
    "print(\"Best Params: {}\".format(gridsearch.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters for the model are to have a penalty of 0.05 and the gamma, or, Kernel coefficient for rbf as .5. I'll run the model again with those hyper parameter specifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1: 0.19910352805089648\n",
      "Test f1: 0.1963921034717495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "svc=SVC(C=.05, gamma=.5, kernel='rbf')\n",
    "svc.fit(x_train, y_train)\n",
    "train_predictions = svc.predict(x_train)\n",
    "test_predictions = svc.predict(x_test)\n",
    "\n",
    "train_f1 = f1_score(y_train, train_predictions, average='macro')\n",
    "test_f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "\n",
    "print(\"Train f1: {}\".format(train_f1))\n",
    "print(\"Test f1: {}\".format(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_svc=svc.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_svc=pd.DataFrame()\n",
    "sub_svc['Id']=Id\n",
    "sub_svc['target']=prediction_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_svc.to_csv(\"sub1_svc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model still didn't do as well as the random forest model or the gradient boosting model. However, it doesn't overfit as bad. But with it's F1 score being less thant .2, it probably doesn't have signifigant predicting power. I will go forward with the Random Forest Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimatley, my best model was the Random Forest. Furthar analysis on the usefulness of the model is below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.34      0.32      0.33        69\n",
      "           2       0.28      0.39      0.33       120\n",
      "           3       0.20      0.26      0.23       108\n",
      "           4       0.86      0.75      0.80       595\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       892\n",
      "   macro avg       0.42      0.43      0.42       892\n",
      "weighted avg       0.66      0.61      0.63       892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)\n",
    "test_predictions = clf.predict(x_test)\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   importance\n",
      "SQBmeaned            0.039992\n",
      "meaneduc             0.037102\n",
      "escolari             0.034236\n",
      "escolari/age         0.032196\n",
      "SQBescolari          0.027202\n",
      "SQBage               0.026808\n",
      "agesq                0.025768\n",
      "phones-per-capita    0.025519\n",
      "walls+roof+floor     0.024449\n",
      "age                  0.023528\n",
      "              importance\n",
      "pisoother            0.0\n",
      "parentesco8          0.0\n",
      "parentesco6          0.0\n",
      "parentesco9          0.0\n",
      "parentesco10         0.0\n",
      "parentesco5          0.0\n",
      "parentesco11         0.0\n",
      "parentesco12         0.0\n",
      "estadocivil1         0.0\n",
      "testing_rez          0.0\n",
      "techootro            0.0\n",
      "paredother           0.0\n",
      "parentesco4          0.0\n",
      "parentesco3          0.0\n",
      "parentesco2          0.0\n",
      "parentesco1          0.0\n",
      "elimbasu5            0.0\n",
      "elimbasu6            0.0\n",
      "planpri              0.0\n",
      "parentesco7          0.0\n"
     ]
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(clf.feature_importances_,\n",
    "                                   index = x_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',  ascending=False)\n",
    "print(feature_importances.head(10))\n",
    "feature_not_importances= pd.DataFrame(clf.feature_importances_,\n",
    "                                   index = x_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',  ascending=True)\n",
    "print(feature_not_importances.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model puts more weight on education, with education, mean education, and schooling scaled by age being the most heavily weighted variables. Variables like the predominant material on the floor/roof, family position, and rubbish disposal are not as important to the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is far from perfect, but that does not mean it has no use. This model does a good job evaluating poverty levels for \"level 4\" households, with an f1_macro score of .80. It does not, however, do a good job at classifying homes in the 1, 2, or 3 category. For that reason, it could be used to initially screen out households which are level 4 for reciving certain types of aid, and because most homes are level 4 in the data set, more resources could be spent evaluating the differences between the needs of the homes remaining. That could save both time and money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
